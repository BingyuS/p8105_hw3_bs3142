---
title: "p8105_hw3_bs3142"
author: "Bingyu Sun"
date: "10/7/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## Problem 1

#### Section 1: Data import

```{r import brfss data}
library(p8105.datasets)
data(brfss_smart2010)
```

#### Section 2: Data Manipulation

Data Cleaning:

* format the data to use appropriate variable names;
* focus on the “Overall Health” topic
* include only responses from “Excellent” to “Poor”
* organize responses as a factor taking levels ordered from “Excellent” to “Poor”
```{r}
brfss_data =
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>% # focus on "Overall Health" topic
  mutate(response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) # convert responses to factor vector with specified orders

str(brfss_data)
```

#### Section 3: Q & A

**1. In 2002, which states were observed at 7 locations?**

```{r}
brfss_data %>%
  filter(year == 2002) %>%
  group_by(locationabbr) %>%
  summarize(n_obs = n_distinct(locationdesc)) %>%
  filter(n_obs == 7)
```

**CT, FL, NC** were observed at 7 locations.

**2. Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.**

```{r}
brfss_data %>%
  group_by(year, locationabbr) %>%
  summarize(n_obs = n_distinct(locationdesc)) %>%
  ggplot(aes(x = year, y = n_obs, color = locationabbr)) + 
    geom_line() +
    labs(
      title = "Number of observations in each state 2002-2010",
      x = "Year",
      y = "Number of observations",
      caption = "Data from brfss_smart2010"
  ) +
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE
  )
```

**3. Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.**

```{r}
brfss_data %>%
  filter(year %in% c("2002", "2006", "2010"), response == "Excellent", locationabbr == "NY") %>%
  spread(key = response, value = data_value) %>%
  group_by(year) %>%
  summarize(
    mean_prop_excellent_NY = mean(Excellent, na.rm = TRUE),
    sd_prop_excellent_NY = sd(Excellent, na.rm = TRUE)
  ) %>%
  knitr::kable(digits = 1)
```


**4. For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time. **

```{r}
brfss_data %>%
  filter(year == 2002) %>%
  group_by(locationabbr, response) %>%
  summarize(mean_prop_response = mean(data_value)) %>%
  ggplot(aes(x = locationabbr, y = mean_prop_response, color = locationabbr)) + 
    geom_point() +
    facet_grid(~response) +
    labs(
      title = "Number of observations in each state 2002-2010",
      x = "Year",
      y = "Number of observations",
      caption = "Data from brfss_smart2010"
  ) +
  viridis::scale_color_viridis(
    name = "Year", 
    discrete = TRUE
  )
```
????????

## Problem 2

#### Section 1: Data import

```{r}
data(instacart)
```

#### Section 2: Data exploration
The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. 
```{r}

```

**Summary**


#### Section 3: Q & A

**1. How many aisles are there, and which aisles are the most items ordered from?**

```{r}
instacart %>%
  summarize(
    n_aisle = n_distinct(aisle_id),
    max_items = max(add_to_cart_order))

instacart %>%
  select(aisle, add_to_cart_order) %>%
  filter(add_to_cart_order == 80)
```

There are **134** aisles. The items **nuts seeds dried fruit** and **packaged vegetables fruits** are ordered the most.

**2. Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.**

```{r}
instacart %>%
  group_by(department, aisle) %>%
  count(aisle) %>%
  ggplot(aes(x = aisle, y = n, color = department)) + 
    geom_point() +
    labs(
      title = "Number of items ordered in each aisle",
      x = "Aisle",
      y = "Number of items ordered",
      caption = "Data from instacart"
  ) +
  viridis::scale_color_viridis(
    name = "Aisle", 
    discrete = TRUE
  )
  
```
?????????

**3. Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.**

```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>%
  count(product_name) %>%
  filter(min_rank(desc(n)) < 2) %>% #get the most popular item in each of the aisles
  knitr::kable(digits = 1)
```

**4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).**

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  select(product_name, order_hour_of_day, order_dow) %>% 
  group_by(product_name, order_dow) %>%
  summarize(mean_order_hour_of_day = mean(order_hour_of_day)) %>%
  mutate(order_dow = c(1, 2, 3, 4, 5, 6, 7)) %>%
  mutate(order_dow = lubridate::wday(order_dow, label = TRUE)) %>% #convert number to week days
  spread(key = order_dow, value = mean_order_hour_of_day) %>%
  knitr::kable(digits = 1)
```


## Problem 3

#### Section 1: Data import

```{r}
data(ny_noaa)
```

#### Section 2: Data exploration
The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. 

```{r}

```

#### Section 3: Q & A


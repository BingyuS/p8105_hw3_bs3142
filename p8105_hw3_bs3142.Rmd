---
title: "p8105_hw3_bs3142"
author: "Bingyu Sun"
date: "10/7/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## Problem 1

#### Section 1: Data import

```{r import brfss data}
library(p8105.datasets) #load library
data(brfss_smart2010) #import brfss data
```

#### Section 2: Data Manipulation

Data Cleaning:

* format the data to use appropriate variable names;
* focus on the “Overall Health” topic
* include only responses from “Excellent” to “Poor”
* organize responses as a factor taking levels ordered from “Excellent” to “Poor”
```{r}
brfss_data =
  brfss_smart2010 %>%
  janitor::clean_names() %>% #clean names
  filter(topic == "Overall Health") %>% # focus on "Overall Health" topic
  mutate(response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))) # convert responses to factor vector with specified orders

str(brfss_data) #view variables
```

#### Section 3: Q & A

**1. In 2002, which states were observed at 7 locations?**

```{r}
brfss_data %>%
  filter(year == 2002) %>% #retain data for 2002
  group_by(locationabbr) %>% #summarize based on locationabbr
  summarize(n_obs = n_distinct(locationdesc)) %>% #get total number of locations observed per state
  filter(n_obs == 7) #get states observed at 7 locations
```

**CT, FL, NC** were observed at 7 locations.

**2. Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.**

```{r, fig.height = 8, dpi = 300}
brfss_data %>%
  group_by(year, locationabbr) %>% #summarize by year and locationabbr
  summarize(n_obs = n_distinct(locationdesc)) %>% #count number of locations in each state
  ggplot(aes(x = year, y = n_obs, color = locationabbr)) + 
    geom_line() +
    labs(
      title = "Number of Locations in Each State 2002-2010",
      x = "Year",
      y = "Number of Observations",
      caption = "Data from brfss_smart2010"
  ) +
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE
  ) #make spagetti plot
```

**3. Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.**

```{r}
brfss_data %>%
  filter(year %in% c("2002", "2006", "2010"), response == "Excellent", locationabbr == "NY") %>% #get NY response for "Excellent" in 2002, 2006, and 2010
  spread(key = response, value = data_value) %>%
  group_by(year) %>% #summarize by years
  summarize(
    mean_prop_excellent_NY = mean(Excellent, na.rm = TRUE),
    sd_prop_excellent_NY = sd(Excellent, na.rm = TRUE)
  ) %>% #get mean and sd
  knitr::kable(digits = 1) #show mean and sd in table
```


**4. For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.**

```{r, fig.height = 12, dpi = 300}
brfss_data %>%
  group_by(year, locationabbr, response) %>%
  summarize(mean_prop_response = mean(data_value)) %>% #get average proportion in each response per state per year
  ggplot(aes(x = year, y = mean_prop_response, color = locationabbr)) + 
    geom_point() +
    facet_grid(~response) + #make panels by response
    labs(
      title = "Mean proportion by response in each state 2002-2010",
      x = "Year",
      y = "Mean proportion",
      caption = "Data from brfss_smart2010"
  ) +
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE
  ) #plot scatterplot showing mean proportion by response
```


## Problem 2

#### Section 1: Data import

```{r}
data(instacart) #load instacart data
```

#### Section 2: Data exploration
The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. 
```{r}

```

**Summary**


#### Section 3: Q & A

**1. How many aisles are there, and which aisles are the most items ordered from?**

```{r}
instacart %>%
  summarize(
    n_aisle = n_distinct(aisle_id),
    max_items = max(add_to_cart_order))

instacart %>%
  select(aisle, add_to_cart_order) %>%
  filter(add_to_cart_order == 80)
```

There are **134** aisles. The items **nuts seeds dried fruit** and **packaged vegetables fruits** are ordered the most.

**2. Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.**

```{r}
instacart %>%
  group_by(department, aisle) %>%
  count(aisle) %>%
  ggplot(aes(x = aisle, y = n, color = department)) + 
    geom_point() +
    labs(
      title = "Number of items ordered in each aisle",
      x = "Aisle",
      y = "Number of items ordered",
      caption = "Data from instacart"
  ) +
  viridis::scale_color_viridis(
    name = "Aisle", 
    discrete = TRUE
  )
  
```
?????????

**3. Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.**

```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>%
  count(product_name) %>%
  filter(min_rank(desc(n)) < 2) %>% #get the most popular item in each of the aisles
  knitr::kable(digits = 1)
```

**4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).**

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  select(product_name, order_hour_of_day, order_dow) %>% 
  group_by(product_name, order_dow) %>%
  summarize(mean_order_hour_of_day = mean(order_hour_of_day)) %>%
  mutate(order_dow = c(1, 2, 3, 4, 5, 6, 7)) %>%
  mutate(order_dow = lubridate::wday(order_dow, label = TRUE)) %>% #convert number to week days
  spread(key = order_dow, value = mean_order_hour_of_day) %>%
  knitr::kable(digits = 1)
```


## Problem 3

#### Section 1: Data import

```{r}
data(ny_noaa)
```

#### Section 2: Data exploration
The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. 

```{r}

```

#### Section 3: Q & A

**1. Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?**

```{r}
tidy_noaa = 
  ny_noaa %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>% #add year, month, and day
  mutate(prcp = prcp/10, 
         tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10) #convert values of prcp to mm and temp to ºC

tidy_noaa %>%
  filter(!is.na(snow)) %>%
  count(snow) %>%
  filter(min_rank(desc(n)) < 4) #get some of the commonly observed values
```

For snowfall, the most commonly observed value was **0mm**, meaning snow days were less frequent compared to non-snow days. For days that did snow, the most commonly observed value was **25mm**, followed by 13mm as the second most frequent value.

**2. Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?**

```{r}
tidy_noaa %>%
  select(-(day:snwd), -tmin) %>%
  filter(month %in% c("01", "07") & !is.na(tmax)) %>% #get Jan and July data, and remove any missing value for tmax
  group_by(year, month, id) %>%
  summarize(mean_tmax = mean(tmax)) %>%
  ggplot(aes(x = year, y = mean_tmax, group = year)) + 
    geom_boxplot() +
    facet_grid(~month) + #create two-panel plots
    labs(
      title = "Average Max Temperature In Each Station Across Years (January VS. July)",
      x = "year",
      y = "Mean Max Temperature (ºC)",
      caption = "Data from ny_noaa"
  ) +
  viridis::scale_color_viridis(
    discrete = TRUE
  )
```

**3. Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.**

```{r}
plot_tmax_tmin = 
  tidy_noaa %>%
  count(tmin)
```

